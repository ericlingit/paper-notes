{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Machine Translation by Jointly Learning to Align and Translate https://arxiv.org/abs/1409.0473\n",
    "\n",
    "Attention? Attention! https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you first look at an image, you first focus on one spot before moving to another spot. This wandering focus is your attention: it spotlights the things that matter, and ignores the things that don't.\n",
    "\n",
    "Just as you can divide an image into sections (spots), and selectively process on just a few of them, you can do the same thing with words, where each word token is its own section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides scanning from spot to spot on an image, your mind also 'expects' (predicts) what it'll see next based on the things it has already seen. For example, when you see a furry texture with a snout, your mind expects to see an animal.\n",
    "\n",
    "With words, if you read 'she is eating a ...', you'd expect that whatever follows is a food of some sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So attention is not just the focus on where to look, but also how the item you're looking at change your expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a new context vector is generated by the decoder before it generates an output. the context vector is calculated from the annotations and the annotation weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when the decoder is trying to generate an output $y_t$, it needs to first figure out which input word to look at. This is done by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The paper](https://arxiv.org/abs/1409.0473)\n",
    "\n",
    "divide the input image into grids\n",
    "\n",
    "use a weight to decide which block to look at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can do the same to a rnn for words:\n",
    "\n",
    "retain all the hidden states, and stack them into a matrix\n",
    "\n",
    "apply a weight to select which hidden state to look at when the decoder is generating a translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the input sentence is run twice: once forwrad, once backward into the rnn. the hidden states of each run are collected and lined up according to the order of the words. \n",
    "\n",
    "now for each word in the sentence, you have 2 hidden states: one that remembers the words before it, and the other remembers the words that come after it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run from the first word $x_1$ to the last word $x_T$ to calculate the forward hidden states:\n",
    "\n",
    "$$(\\overrightarrow{h_1}, \\dots, \\overrightarrow{h_{T_x}})$$\n",
    "\n",
    "The backward run from the last word $x_T$ to the first word $x_1$ to calculate the backward hidden states:\n",
    "\n",
    "$$(\\overrightarrow{h_{T_x}}, \\dots, \\overrightarrow{h_1})$$\n",
    "\n",
    "This backward hidden states vector is reversed so that the order of inputs $x_t$ are aligned with the forward hidden states vector:\n",
    "\n",
    "$$(\\overleftarrow{h_1}, \\dots, \\overleftarrow{h_{T_x}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the forward and backward hidden states for each input word are stacked (concatenated) together to form its __annotation__ $h_j$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_j = \\left[\n",
    "\\overrightarrow{h}_j^\\top ; \\overleftarrow{h}_j^\\top \\right]^\\top$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since RNNs only retain recent knowledge, the combined hidden states at each step will have information for both the preceding words, and the following words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find which words (annotations) to pay attention to, we need some sort of mask to highlight certain words, and diminish others before the decoder can decide what to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mask is the context vector $c_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context vector __for the current step__ $c_i$ is the sum of the current step's alignment weight $a_{ij}$ multiplied by all annotations $h_j$.\n",
    "\n",
    "_Alignment_ means matching the input word (annotation) to the output word. The alignment is based on the decoder's previous hidden state $s_{i-1}$ and the annotation $h_j$.\n",
    "\n",
    "__$a_{ij}$ selects which words to pay attention to by highlighting some words, and diminishing others__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### self-attention == association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
