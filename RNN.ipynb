{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "Andrej Karpathy blog: The Unreasonable Effectiveness of Recurrent Neural Networks http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "Stanford cs231n (spring 2017) lecture 10: Recurrent Neural Networks https://www.youtube.com/watch?v=6niqTuYFZLQ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context:\n",
    "\n",
    "Neural networks like CNNs typically require some fixed-size input and produce a fixed-size output (see one-to-one below).\n",
    "\n",
    "RNNs can operate on every item of a sequence; so the length of that sequence can very in size. Not just the input, the output can also vary in size.\n",
    "\n",
    "- One to many: eg, given an input image, produce a sequence of output words (image captioning)\n",
    "- Many to one: eg, given an input sequence of words, produce a single classification of output (sentiment classification)\n",
    "- Many to many: eg, given an input sequence of english words, produce a sequence of french words (machine translation); or given an input sequence of video frames, produce a classification output (whether there's a cat in the frame) for each input frame.\n",
    "\n",
    "You can also iterate over fixed-sized inputs on an RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./img/rnn-in-out-size.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does a RNN work?\n",
    "\n",
    "Like a static variable in a class that gets updated every time some method is called, the hidden state in a RNN is updated by a new input. The updated hidden state is fed back into the model the next time it reads a new input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./img/rnn-recurrence-formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating new state and _recurrence_:\n",
    "\n",
    "For example: for every word in a sentence, run the word through the RNN function (the input word can be a one-hot encoded vector):\n",
    "\n",
    "$$h_t = \\tanh ( W_{hh} h_{t-1} + W_{xh} x_t )$$\n",
    "\n",
    "At the first step ($t = 1$), the function takes the first word $x_1$, and a hidden state $h$ as inputs. Since the hidden state is initialized to a vector of zeroes, the first hidden state is essentially the tanh of the first word (tanh is applied element-wise; it squashes each value to betwenn -1 and 1):\n",
    "\n",
    "$$h_1 = \\tanh ( W_{xh} x_1 )$$\n",
    "\n",
    "![img](./img/rnn-step1.png)\n",
    "\n",
    "The second word $x_2$ together with previous step's hidden state $h_1$ are fed into the same function to produce a new hidden state $h_2$:\n",
    "\n",
    "$$h_2 = \\tanh ( W_{hh} h_1 + W_{xh} x_2 )$$\n",
    "\n",
    "![img](./img/rnn-step2.png)\n",
    "\n",
    "This process is repeated until the end of the sentence.\n",
    "\n",
    "![img](./img/rnn-step3.png)\n",
    "\n",
    "Both $x_t$ and $h_t$ have their own set of weights $W_{xh}$ and $W_{hh}$ (as a fully connected layer) that remain unchanged during the forward pass.\n",
    "\n",
    "![img](./img/rnn-step3-weights.png)\n",
    "\n",
    "If you want to produce an output $y_t$ at each step, you can introduce another set of weights $W_{hy}$ for the calculation:\n",
    "\n",
    "$$y_t = W_{hy} \\times h_t $$\n",
    "\n",
    "The last hidden state $h_T$ at the end of the sentence can be considered to be a summary of the input sentence.\n",
    "\n",
    "In code, it looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    '''A single recurrent cell'''\n",
    "    def step(self, x):\n",
    "        '''Update the hidden state'''\n",
    "        self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "        # Optional: compute the output vector\n",
    "        y = np.dot(self.W_hy, self.h)\n",
    "        return y\n",
    "\n",
    "rnn = RNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: `np.dot(a, b)` is the [dot-product](https://en.wikipedia.org/wiki/Dot_product#Algebraic_definition) of 2 vectors (__not__ element-wise!).\n",
    "\n",
    "`np.tanh` is applied element-wise.\n",
    "\n",
    "Notice that there are __three sets of weights__! $W_{hh}$ (`self.W_hh`) for the hidden state; $W_{xh}$ (`self.W_xh`) for the input; and $W_{hy}$ (`self.W_hy`) for the output.\n",
    "\n",
    "The hidden state `self.h` is initialized to a vector of zeros.\n",
    "\n",
    "`np.tanh` function implements a non-linearity that squashes the activations to the range `[-1, 1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each step in a sequence, you'd run the line below to update the hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.step(x) # x is an input vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can daisy-chain (stack) RNN models so that the output of one cell becomes the input of a downstream cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = rnn.step(x)\n",
    "y2 = rnn2.step(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
