{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "Andrej Karpathy blog: The Unreasonable Effectiveness of Recurrent Neural Networks http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "Stanford cs231n (spring 2017) lecture 10: Recurrent Neural Networks https://www.youtube.com/watch?v=6niqTuYFZLQ\n",
    "\n",
    "Visualizing and Understanding Recurrent Networks https://arxiv.org/abs/1506.02078"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context:\n",
    "\n",
    "Neural networks like CNNs typically require some fixed-size input, and produce a fixed-size output (see one-to-one in figure below).\n",
    "\n",
    "RNNs can operate on every item of a sequence; so the length of that sequence can very in size. The output can also vary in size.\n",
    "\n",
    "- One to many: eg, given an input image, produce a sequence of words that describes it (image captioning)\n",
    "- Many to one: eg, given an input sequence of words, produce a single label for the text (sentiment classification)\n",
    "- Many to many: eg, given a sequence of English words, produce a sequence of French words (machine translation); or given a sequence of video frames, produce a sentence describing the scene.\n",
    "\n",
    "You can also iterate over fixed-sized inputs on an RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./img/rnn-in-out-size.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does a RNN work?\n",
    "\n",
    "Like a static variable in a class that gets updated every time some method is called, the hidden state in a RNN is updated by a new input. The updated hidden state is fed back into the model the next time it reads a new input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./img/rnn-recurrence-formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating new state and _recurrence_:\n",
    "\n",
    "For example: for every word in a sentence, run the word through the RNN function (the input word can be a one-hot encoded vector):\n",
    "\n",
    "$$h_t = \\tanh ( W_{hh} h_{t-1} + W_{xh} x_t )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first step ($t = 1$), the function takes the first word $x_1$, and a hidden state $h$ as inputs. Since the hidden state is initialized to a vector of zeroes, the first hidden state is essentially the tanh of the first word (tanh is applied element-wise; it squashes each value to betwenn -1 and 1):\n",
    "\n",
    "$$h_1 = \\tanh ( W_{xh} x_1 )$$\n",
    "\n",
    "![img](./img/rnn-step1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second word $x_2$ together with previous step's hidden state $h_1$ are fed into the same function to produce a new hidden state $h_2$:\n",
    "\n",
    "$$h_2 = \\tanh ( W_{hh} h_1 + W_{xh} x_2 )$$\n",
    "\n",
    "![img](./img/rnn-step2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is repeated until the end of the sentence.\n",
    "\n",
    "![img](./img/rnn-step3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both $x_t$ and $h_t$ have their own set of weights $W_{xh}$ and $W_{hh}$ (as a fully connected layer) that remain unchanged during the forward pass.\n",
    "\n",
    "![img](./img/rnn-step3-weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to produce an output $y_t$ at each step, you can introduce another set of weights $W_{hy}$ for the calculation:\n",
    "\n",
    "$$y_t = W_{hy} \\times h_t $$\n",
    "\n",
    "![img](./img/rnn-step3-outputs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    '''A single recurrent cell'''\n",
    "    def step(self, x):\n",
    "        '''Update the hidden state'''\n",
    "        self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "        # Optional: compute the output vector\n",
    "        y = np.dot(self.W_hy, self.h)\n",
    "        return y\n",
    "\n",
    "rnn = RNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: `np.dot(a, b)` is the [dot-product](https://en.wikipedia.org/wiki/Dot_product#Algebraic_definition) of 2 vectors (__not__ element-wise!).\n",
    "\n",
    "`np.tanh` is applied element-wise.\n",
    "\n",
    "Notice that there are __three sets of weights__! $W_{hh}$ (`self.W_hh`) for the hidden state; $W_{xh}$ (`self.W_xh`) for the input; and $W_{hy}$ (`self.W_hy`) for the output.\n",
    "\n",
    "The hidden state `self.h` is initialized to a vector of zeros.\n",
    "\n",
    "`np.tanh` function implements a non-linearity that squashes the activations to the range `[-1, 1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each step in a sequence, you'd run the line below to update the hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.step(x) # x is an input vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can daisy-chain (stack) RNN models so that the output of one cell becomes the input of a downstream cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = rnn.step(x)\n",
    "y2 = rnn2.step(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each prediction, there's the accompanying loss (usually softmax loss):\n",
    "\n",
    "![img](./img/rnn-step3-outputs-loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final loss is the sum of all individual loss at each step:\n",
    "\n",
    "![img](./img/rnn-step4-final-loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the backwards pass, the loss gradient flows through each time-step, and each step will compute the local gradient for weights $W_{hh}$, $W_{xh}$, and $W_{hy}$. Which are then summed for the final gradient for the weights $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a __many-to-one__ model, the last hidden state $h_T$ at the end of the sentence can be considered a summary of the input sentence.\n",
    "\n",
    "Whereas for a __one-to-many__ model, the input is an initializer for step 1 of the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __sequence-to-sequence__ model (eg, neural machine translation) is basically a many-to-one model placed before a one-to-many model.\n",
    "\n",
    "It operates in 2 stages:\n",
    "1. the model upfront __encodes__ the input sequence of words to a single summary vector. That vector is the hidden state of the last step of the model.\n",
    "1. the model downstream __decodes__ that vector into a sequence of words in another language.\n",
    "\n",
    "![img](./img/rnn-seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we have a character-level model that predicts what the next letter should be given an input letter. Assume the vocabulary consists of only 4 letters: h, e, l, o; and we're training the model to predict the word 'hello'.\n",
    "\n",
    "The characters are first converted to a 4-element, one-hot vector:\n",
    "\n",
    "![img](./img/rnn-char-seq-inputs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "The input vector multiplies by the input weight matrix $W_{xh}$ to get the first hidden state. That hidden state is then multiplied by an output weight matrix $W_{hy}$ to produce a list of scores for each letter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./img/rnn-char-seq.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first letter 'h', the correct next letter should be 'e', but it gave 'o' a higher score. In this case, we'd use a softmax loss to quantify (a scalar) how wrong the prediction is, and that loss's gradient will be fed back into the cell during the backwards pass. This process repeats during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "At test time, each input's score is converted to a probability distribution by a softmax function. A prediction is then __sampled__ from that distribution. That prediction is then fed back for the next time-step:\n",
    "\n",
    "![img](./img/rnn-eg-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this process repeats:\n",
    "\n",
    "![img](./img/rnn-eg-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "Why __sample__ from the distribution? Why not just take the character with the highest score (argmax)?\n",
    "\n",
    "- Sometimes you do take the argmax. The advantage with sampling is that you get variety in your outputs so that you don't always end up with the same output given the same input. eg, the same image can be captioned in a few different ways by the model.\n",
    "\n",
    "During test time, when the first prediction is made, can you feed the softmax score into the next round (instead of using the one-hot vector)?\n",
    "\n",
    "- No, because the softmax scores look very different from what the model saw during training. This can cause bad outputs.\n",
    "- The other problem is that the using a dense vector as an input can be computationally expensive. If your vocabulary size is 10,000, then the input vector becomes a dense softmax vector of 10,000 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the forward pass, you're stepping through time to compute the loss. During the backward pass, you're stepping backwards through time to compute the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the input sequence is very long? Like Wikipedia-sized long? You can't just run through all wikipedia text forward and backward to produce one gradient update; you'll run out of memory and never converge because it's so slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as you'd make a gradient update after every few images for a CNN, you can do the same thing (mini-batch) to gradient updates in a RNN:  you run a gradient update once every few words (say, 100 words). This is known as a __truncated backpropagation through time__.\n",
    "\n",
    "![img](./img/rnn-truncated-bptt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hidden state visualized\n",
    "\n",
    "What exactly does it 'remember'?\n",
    "\n",
    "[This paper](https://arxiv.org/abs/1506.02078) selects one number from the hidden state vector, and see which characters cause a spike in activation when an input sequence is fed into the model.\n",
    "\n",
    "Most elements from the hidden state vector aren't easily interpretable:\n",
    "\n",
    "![img](./img/rnn-interpret1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But some elements activate in a more interpretable way:\n",
    "\n",
    "Quote detection:\n",
    "\n",
    "![img](./img/rnn-interpret2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line break:\n",
    "\n",
    "![img](./img/rnn-interpret3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`if` statement conditions:\n",
    "\n",
    "![img](./img/rnn-interpret4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The take-away is that even though the model was trained to predict the next character, it also learned useful structural rules of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image captioning\n",
    "\n",
    "A CNN distills the image down to a summary vector $v$, it becomes another input to the hidden state formula. $v$ also has its own weights $W_{ih}$.\n",
    "\n",
    "![img](./img/rnn-img-caption.png)\n",
    "\n",
    "The initial input is a special `<START>` token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, the hidden state was calculated like this:\n",
    "\n",
    "$$h_t = \\tanh ( W_{hh} h_{t-1} + W_{xh} x_t )$$\n",
    "\n",
    "Now we have an image vector $v$ and its weights $W_{ih}$ to account for:\n",
    "\n",
    "$$h_t = \\tanh ( W_{hh} h_{t-1} + W_{xh} x_t + W_{ih} v)$$\n",
    "\n",
    "__It's important to note that the image vector is not used as the input $x_t$__!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, the labels must have special tokens marking the `<START>` and `<END>` of the sentence. This tells the network to stop generating words whenever it has sampled an `<END>` token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![img](./img/rnn-img-caption2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./img/rnn-img-caption3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example results:\n",
    "\n",
    "![img](./img/rnn-img-caption-eg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad results:\n",
    "\n",
    "![img](./img/rnn-img-caption-eg2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image captioning with attention\n",
    "\n",
    "instead of outputting a single summary vector, it outputs a grid of vectors. imagine the image is divided into grids of 9 squares. then the output matrix will have 9 vectors, each corresponding to its own location in the grid.\n",
    "\n",
    "besides sampling the output vocab, the model also produces a distribution of image locations that it wants to look. This distribution can be seen as the attention of the model (ie, where it is looking at).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
