{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "Andrej Karpathy blog: The Unreasonable Effectiveness of Recurrent Neural Networks http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "Stanford cs231n (spring 2017) lecture 10: Recurrent Neural Networks https://www.youtube.com/watch?v=6niqTuYFZLQ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context:\n",
    "\n",
    "Neural networks like CNNs typically require some fixed-size input and produce a fixed-size output. \n",
    "\n",
    "RNNs can operate on every item of a sequence; so the length of that sequence can very in size. Not just the input, the output can also vary in size.\n",
    "\n",
    "This has several advantages: you can have various combo of input size vs output size.\n",
    "\n",
    "You can also iterate over fixed-sized inputs on an RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./img/rnn-in-out-size.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does a RNN work?\n",
    "\n",
    "Like a static variable in a class that gets updated every time some method is called, a hidden state in a RNN cell retains some of the things it has seen, and is updated by new inputs.\n",
    "\n",
    "The __Recurrent__ part of RNN:\n",
    "\n",
    "Example: for every word in a sentence, run the word through the RNN function.\n",
    "\n",
    "The function takes the first word `x`, and a hidden state `h` (initialized to 0) as inputs, and produce a new updated hidden state `h1`. `h1` is then fed into the same function with the next word `x1` to produce a new hidden state `h2`. This process is repeated until the end of the sentence.\n",
    "\n",
    "Both `x` and `h` has their own set of weights (represented as a fully connected layer).\n",
    "\n",
    "If you want to produce an output at each step, you can introduce another set of weights for the calculation of `y`.\n",
    "\n",
    "That last hidden state can be considered to be the summary of the input sentence.\n",
    "\n",
    "As a program, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    '''A single recurrent cell'''\n",
    "    def step(self, x):\n",
    "        # update the hidden state\n",
    "        self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "        # compute the output vector (if needed)\n",
    "        y = np.dot(self.W_hy, self.h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are __three sets of weights__! One (`W_hh`) for the hidden state; one (`W_xh`) for the input; and one (`W_hy`) for the output.\n",
    "\n",
    "The hidden state `self.h` is initialized with a zero vector.\n",
    "\n",
    "`np.tanh` function implements a non-linearity that squashes the activations to the range `[-1, 1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each step in a sequence, you'd run the line below to update the hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.step(x) # x is an input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
